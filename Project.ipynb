{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for training `QDA` :\n",
    "- takes the inputs : $X_{app}$ (training input matrix $n, p$), $z_{app}$ (binary class indicator matrix $n, K$), $m_{prior}$ (Gaussian prior expectation matrix $K, p$), $df_{exp}$ (Gaussian prior shrinkage parameter), $S_{prior}$ (covariance matrix $p, p, K$ for the inverse -Wishart prior), $df_{cov}$ (degree of freedom of the inverse-Wishart prior)\n",
    "- provides : $pi$ (vector $K, 1$ of class proportions), $mu$ (estimated expectation matrix $K, p$), $Sig$ (estimated covariance matrices $p, p, K$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QDA(Xapp, zapp, mprior, df_exp, Sprior, df_cov):\n",
    "    n, p, K =  Xapp.shape[0], Xapp.shape[1], zapp.shape[1]\n",
    "    pi = np.zeros(K)\n",
    "    mu = np.zeros((K, p))\n",
    "    B = np.zeros((p, p, n))\n",
    "    Sig = np.zeros((p, p, K))\n",
    "    \n",
    "    pi = np.mean(zapp, axis=0)\n",
    "    for j in range(K):\n",
    "        mu[j, :] = (sum(zapp[i, j] * Xapp[i, :] for i in range(n)) \n",
    "                    + df_exp * mprior[j, :]) / (sum(zapp[:, j]) + df_exp)\n",
    "        \n",
    "        for i in range(n):\n",
    "            temp = (Xapp[i, :] - mprior[j, :]).reshape((p, -1))\n",
    "            B[:, :, i] = temp @ temp.T\n",
    "        \n",
    "        Sig[:, :, j] = (sum((zapp[i, j] * B[:, :, i]) for i in range(n)) + \n",
    "                        df_exp * (mu[j, :] - mprior[j, :]) @ (mu[j, :] - mprior[j, :]).T + \n",
    "                        Sprior[:, :, j]) / (sum(zapp[:, j]) + df_cov + p + 2)\n",
    "        \n",
    "    return pi, mu, Sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for evaluating test set `evaluation` :\n",
    "- takes the inputs : $X_{tst}$ (test input matrix $n, p$), $pi$ (vector $K, 1$ of class proportions), $mu$ (estimated expectation matrix $K, p$), $Sig$ (estimated covariance matrices $p, p, K$)\n",
    "- provides : $prob$ (matrix $n, K$ of estimated class posterior probabilities) and $pred$ (vector $n, 1$ of corresponding decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(Xtst, pi, mu, Sig):\n",
    "    n, p, K = Xtst.shape[0], Xtst.shape[1], len(pi)\n",
    "    prob = np.zeros((n, K))\n",
    "    pred = np.zeros(n)\n",
    "    \n",
    "    f = lambda x, mu, Sig: (2 * np.pi)**(-n/2) * np.linalg.det(Sig)**(-1/2) * np.exp((-1/2) * \n",
    "                                                                                     (x - mu).T\n",
    "                                                                                     @ np.linalg.pinv(Sig)\n",
    "                                                                                     @ (x - mu))\n",
    "   \n",
    "    for j in range(K):\n",
    "        for i in range(n):\n",
    "            total = sum(pi[k] * f(Xtst[i, :], mu[k, :], Sig[:, :, k]) for k in range(K))\n",
    "            prob[i, j] = pi[j] * f(Xtst[i, :], mu[j, :], Sig[:, :, j]) / total\n",
    "        \n",
    "    pred = np.argmax(prob, axis=1)\n",
    "    \n",
    "    return prob, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a `data_processing` function to test different dataset. The function does respectively :\n",
    "- Read the dataset by the `pandas` library and transform it to the one in form of the `numpy` library in order to facilitate our next works.\n",
    "- Apply PCA filtering process to the dataset if necessary.\n",
    "- Sort all classes of the dataset and display their enumeration\n",
    "- Separate the dataset into training and test ones by `cut_off` coefficient predefined at `0.9`\n",
    "- Calculate the prior expectation and variance based on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(file_url, n_components_pca=None, cut_off=0.9):\n",
    "    data = pd.read_csv(file_url, header=0)\n",
    "    data = data.to_numpy()\n",
    "    \n",
    "    if n_components_pca is not None:\n",
    "        data_y = data[:, -1]\n",
    "        data_X = data[:, :-1]\n",
    "        pca = PCA(n_components=n_components_pca)\n",
    "        data_X = pca.fit_transform(data_X)\n",
    "        data = np.hstack((data_X, data_y.reshape((-1,1))))\n",
    "    \n",
    "    classes = sorted(collections.Counter(list(data[:, -1])).keys())\n",
    "    classes_with_index = {}\n",
    "    \n",
    "    K = len(classes)\n",
    "    n, p = data.shape[0], data.shape[1] - 1\n",
    "    for i_class in range(K):\n",
    "        classes_with_index[i_class] = classes[i_class]\n",
    "        data[data[:, -1] == classes[i_class], -1] = i_class\n",
    "    print(\"Ours classes are enumerated as bellow : \\n\", classes_with_index)\n",
    "    \n",
    "    cut_off = int(cut_off*data.shape[0])\n",
    "    if cut_off > data.shape[0]:\n",
    "        raise IndexError(\"list index out of range\")\n",
    "    else:\n",
    "        Xapp = data[0:cut_off, :-1]\n",
    "        Xtst = data[cut_off:, :-1]\n",
    "\n",
    "        Zapp = np.zeros((cut_off, K))\n",
    "        Ztst = np.zeros((n - cut_off, K))\n",
    "\n",
    "        for i in range(cut_off):\n",
    "            Zapp[i, int(data[i, -1])] = 1\n",
    "        for i in range(cut_off, n):\n",
    "            Ztst[i - cut_off, int(data[i, -1])] = 1\n",
    "    \n",
    "    mprior = np.zeros((K, p))\n",
    "    Sprior = np.zeros((p, p, K))\n",
    "    for j in range(K):\n",
    "        temp = data[data[:, -1] == j, :-1]\n",
    "        temp = temp.astype(float)\n",
    "        mprior[j, :] = temp.mean(axis = 0)\n",
    "        Sprior[:, :, j] = np.cov(temp, rowvar=False)\n",
    "    return Xapp, Zapp, Xtst, Ztst, mprior, Sprior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `optdigits` dataset has many zero elements that make itselft correlated. That's why the result from the inital training datatset is badly evaluated. We then apply a PCA filtering process with number of principal components depending on the `cut_off` coefficient (rank of training set) in order to eliminate the correlated ones. As a result, we get much better performance. <br>\n",
    "For the choice of the shrinkage parameter $df_{exp}$ and the degree of freedom $df_{co}v$, we a priori choose $df_{exp} = 10$ and $df_{cov} = 300$ for this dataset and also the next ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ours classes are enumerated as bellow : \n",
      " {0: 0.0, 1: 1.0, 2: 2.0, 3: 3.0, 4: 4.0, 5: 5.0, 6: 6.0, 7: 7.0, 8: 8.0, 9: 9.0}\n",
      "\n",
      "\n",
      " [[27  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 24  0  0  0  0  0  0  3  0]\n",
      " [ 0  0 27  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 21  0  1  0  0  5  0]\n",
      " [ 0  0  0  0 27  0  0  0  3  0]\n",
      " [ 0  0  0  0  0 29  0  0  0  1]\n",
      " [ 0  0  0  0  0  0 28  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 28  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 27  0]\n",
      " [ 0  0  0  0  1  1  0  2  1 25]]\n",
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      0.89      0.94        27\n",
      "           2       1.00      1.00      1.00        27\n",
      "           3       1.00      0.78      0.88        27\n",
      "           4       0.96      0.90      0.93        30\n",
      "           5       0.94      0.97      0.95        30\n",
      "           6       1.00      1.00      1.00        28\n",
      "           7       0.93      1.00      0.97        28\n",
      "           8       0.69      1.00      0.82        27\n",
      "           9       0.96      0.83      0.89        30\n",
      "\n",
      "    accuracy                           0.94       281\n",
      "   macro avg       0.95      0.94      0.94       281\n",
      "weighted avg       0.95      0.94      0.94       281\n",
      "\n",
      "\n",
      "\n",
      " 0.9489853467238384\n"
     ]
    }
   ],
   "source": [
    "df_exp_opt = 10\n",
    "df_cov_opt = 300\n",
    "#cuts_off = np.array([0.9, 0.91, 0.92, 0.93, 0.94, 0.95])\n",
    "#prediction = np.zeros(4)\n",
    "\n",
    "#for i in range(len(cuts_off))\n",
    "Xapp_opt, Zapp_opt, Xtst_opt, Ztst_opt, mprior_opt, Sprior_opt = data_processing(\n",
    "                                            \"dataset/optdigits.csv\", n_components_pca=52, cut_off=0.95)\n",
    "pi_opt, mu_opt, Sig_opt = QDA(Xapp_opt, Zapp_opt, mprior_opt, df_exp_opt, Sprior_opt, df_cov_opt)\n",
    "prob_opt, pred_opt = evaluation(Xtst_opt, pi_opt, mu_opt, Sig_opt)\n",
    "#prediction[i] = precision_score(np.argmax(Ztst_opt, axis=1), pred_opt, average='weighted')\n",
    "\n",
    "print('\\n\\n', confusion_matrix(np.argmax(Ztst_opt, axis=1), pred_opt))\n",
    "print('\\n\\n', classification_report(np.argmax(Ztst_opt, axis=1), pred_opt))\n",
    "print('\\n\\n', precision_score(np.argmax(Ztst_opt, axis=1), pred_opt, average='weighted'))\n",
    "\n",
    "#plt.plot(cuts_off, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xapp_pag, Zapp_pag, Xtst_pag, Ztst_pag, mprior_pag, Sprior_pag = data_processing(\"dataset/page-blocks.csv\")\n",
    "\n",
    "df_exp_pag = 10\n",
    "df_cov_pag = 300\n",
    "\n",
    "pi_pag, mu_pag, Sig_pag = QDA(Xapp_pag, Zapp_pag, mprior_pag, df_exp_pag, Sprior_pag, df_cov_pag)\n",
    "prob_pag, pred_pag = evaluation(Xtst_pag, pi_pag, mu_pag, Sig_pag)\n",
    "\n",
    "print('\\n\\n')\n",
    "print(confusion_matrix(np.argmax(Ztst_pag, axis=1), pred_pag))\n",
    "print('\\n\\n')\n",
    "print(classification_report(np.argmax(Ztst_pag, axis=1), pred_pag))\n",
    "print('\\n\\n')\n",
    "print(precision_score(np.argmax(Ztst_pag, axis=1), pred_pag, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xapp_sat, Zapp_sat, Xtst_sat, Ztst_sat, mprior_sat, Sprior_sat = data_processing(\"dataset/satimage.csv\")\n",
    "\n",
    "df_exp_sat = 10\n",
    "df_cov_sat = 300\n",
    "\n",
    "pi_sat, mu_sat, Sig_sat = QDA(Xapp_sat, Zapp_sat, mprior_sat, df_exp_sat, Sprior_sat, df_cov_sat)\n",
    "prob_sat, pred_sat = evaluation(Xtst_sat, pi_sat, mu_sat, Sig_sat)\n",
    "\n",
    "print('\\n\\n')\n",
    "print(confusion_matrix(np.argmax(Ztst_sat, axis=1), pred_sat))\n",
    "print('\\n\\n')\n",
    "print(classification_report(np.argmax(Ztst_sat, axis=1), pred_sat))\n",
    "print('\\n\\n')\n",
    "print(precision_score(np.argmax(Ztst_sat, axis=1), pred_sat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xapp_seg, Zapp_seg, Xtst_seg, Ztst_seg, mprior_seg, Sprior_seg = data_processing(\"dataset/segment.csv\",\n",
    "                                                                                n_components_pca=17)\n",
    "\n",
    "df_exp_seg = 10\n",
    "df_cov_seg = 300\n",
    "\n",
    "pi_seg, mu_seg, Sig_seg = QDA(Xapp_seg, Zapp_seg, mprior_seg, df_exp_seg, Sprior_seg, df_cov_seg)\n",
    "prob_seg, pred_seg = evaluation(Xtst_seg, pi_seg, mu_seg, Sig_seg)\n",
    "\n",
    "print('\\n\\n')\n",
    "print(confusion_matrix(np.argmax(Ztst_seg, axis=1), pred_seg))\n",
    "print('\\n\\n')\n",
    "print(classification_report(np.argmax(Ztst_seg, axis=1), pred_seg))\n",
    "print('\\n\\n')\n",
    "print(precision_score(np.argmax(Ztst_seg, axis=1), pred_seg, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.9766310415063318 1;p+20 ll\n",
    "# 0.9758926103474012 10;p+20\n",
    "# 0.9671482548426473 10;p+300 \n",
    "# v reprensent the least informative -> v high -> cov lower -> more \"confident\" ????\n",
    "# high df_cov -> Sig =~~ sampling covariance matrix (in this case Sprior)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
